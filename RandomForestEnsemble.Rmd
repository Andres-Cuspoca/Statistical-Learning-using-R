---
title: "Random Forests in R"
output:
  html_document: default
  html_notebook: default
---


## Random Forests 

Random Forests are a __Ensembling__ technique which is similar to a famous Ensemble technique called  *__Bagging__* but a different tweak in it. In Random Forests the idea is to __decorrelates__ the several trees which are generated on the different bootstrapped samples from training Data.And then we simply reduce the Variance of the Trees by averaging them. 

Averaging the Trees helps us to reduce the variance and also improve the Perfomance of Decision Trees on Test Set and eventually avoid Overfitting.

The idea is to build a lots of Trees in such a way so as to make the *Correlation* between the Trees smaller.

Another major difference is that we only consider a Random subset of predictors $X_i$,say $m$ each time we do a split on training examples.Whereas usually in Trees we consider all the predictors while doing a split and choose best amongst them. Typically  $m = \sqrt{p} $ where $p$ are the number of predictors.

Now it seems crazy to throw away lots of predictors but it actually makes sense because the effect of doing is that each tree uses different predictors to split data at different times.

*So by doing this trick of throwing away Predictors, we have decorrelated the Trees and the resulting average seems a little better. *



-----


## Implementing Random Forests in R

Loading the Packages

```{r,warning=FALSE,message=FALSE}
require(randomForest)
require(MASS)#Package which contains the Boston housing dataset

dim(Boston)
attach(Boston)
set.seed(101)

```

