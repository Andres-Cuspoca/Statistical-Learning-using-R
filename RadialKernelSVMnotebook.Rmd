---
title: "Radial Kernel SVM"
output: html_notebook
---


###Radial Kernel Support Vector Machine

This article will be all about how to separate non linear data using a *__non-linear decision boundary__ *, which cannot be simply separated by a linear separator.

It is often encountered that Linear Separators and boundaries fail because of the non linear interactions in the data and the non linear dependence between the features in feature space.

The trick here is that ,we will do __feature expansion__.

So how we solve this problem is via doing a non linear transformation on the features($X_i$) and converting them to a higher dimentional space called a feature space.Now by this transformation we are able to saperate non linear data using a non linear decision boundary.

Non linearities can simply be added by using higher dimention terms such as square and cubic polynomial terms .

$$y_i = \beta_0 + \beta_1X_{1} \ +  \beta_2X_{1}^2 +  \beta_3X_2 + \beta_4X_2^2 +  \beta_5X_2^3 .... = 0  $$ is the equation of the non linear hyperplane which is generated if we use *higher degree polynomials* terms to fit to data to get a non linear decision boundary.

What we are actually doing is that we are fitting a SVM is an enlarged space.We enlarge the space of features by doing non linear transformations.

But the problem with __Polynomials__ are that in higher dimentions i.e when having lots of predictors it gets wild and generally overfits at higher degrees of polynomials.

Hence there is another elegant way of adding non linearities in SVM is by the use of *__Kernel trick__*.

-----------------


####Kernel Function

Kernel function is a function of form--
$$K(x,y) = (1 + \sum_{j=1}^{p} x_{ij}. y_{ij})^d$$ where d = degree of polynomial.

Now the type of Kernel function we are going to use here is a __Radial kernel__.

The radial kernel is of form:
$$k(x,y) = \exp(- \  \gamma \ \sum_{j=1}^{p}(x_{ij} - y_{ij})^2) $$
Here $\gamma$ is a hyper parameter or a tuning parameter which accounts for the smoothness of the decision boundary and controls the variance of the model.

If $\gamma$ is very large then we get quiet fluctuating and wiggly decision boundaries which accounts for high variance and overfitting.

If $\gamma$ is small , the decison line or boundary is smoother and has low variance.

