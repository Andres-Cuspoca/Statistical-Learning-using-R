---
title: "Principal Components Analysis"
output: html_notebook
author : "Anish Singh Walia"
---

##Unsupervised Learning

__Unsupervised__ learning is a machine learning technique in which the dataset has no target variable or no response value-$Y$.The data is unlabelled.
Simply saying,there is no target value to supervise the learning process of a learner unlike in __Supervised__ learning where we had training examples which had both input variables $X_i$ and target variable-$Y$ -{$(x_i,y_i)$} and by looking and learning from the training examples the learner used to generate a *mapping* function(also called a __hypothesis__) $f : x_i-> y$ which mapped $x_i$ values to $y$ and learned the relationship between input variables and target variable so that we could generalize it to some random unseen test examples and predict the target value.

The best example of unsupervised learning is when a small child given some unlabelled pictures of cats and dogs , so by only looking at the structural similarities and disimilarities between the images , he classifies one as a dog and other as cat.

There are lots of examples of unsupervised learning around us.

__Unsupervised learning is mostly used as a preprocessing tool for supervised learning__.e.g-like PCA could be used to select a linear combination of predictors-$X_i$ which explains the most variability in the data , and reduce a high-dimentional dataset to a lower dimentional view with only most relevant and important features which can be used as inputs in a supervised learning model.

e.g If we have a dataset with 100 predictors and we wanted to generate a model,it would be highly inefficient to use all those 100 predictors because that would increase the variance and complexity of the model and which in turn would lead to __overfitting__.Instead what PCA does is find 10 most correlated variables and linearly combine them to generate a principal component -$Z_1$. 

----------------------


##Principal Components Analysis

PCA introduces a lower-dimentional representation of the dataset.It finds a sequence of linear combination of the variables called the principal components-$Z_1,Z_2...Z_m$ that explain the maximum variance in the data and are mutually uncorrelated.

What we try to do is find most relevant set of variables and simply linearly combine the set of variables into a single variable-$Z_m$.

1)The first principal component $PC_1$ has the highest variance across data.

2)The second principal component $PC_2$ is uncorrelated with $PCA_1$ which also has high variance.

We have tons of correlated variables in a high dimentional dataset and what PCA tries to do is pair and combine them to a set of some important variables that summarize all information in the data.

PCA will give us new set of variables called principal components which could be further be used as inputs in a supervised learning model.
So now we have lesser and most important set of variables paired together to form a new single variable which explains most variance in data.
This technique is often termed as __Dimentionality Reduction__ which is famous technique to do feature selection and use only relevant features in the Model.


###Details

1) we have a set of input vectors $x_1,x_2,x_3.....x_p$ with $n$ observations in dataset.

2) The $1^{st}$ principal component $Z_1$ of a set of features is the __normalized linear combination__ of the features $x_1,x_2....x_p$.
    $$Z_1=z_{i1} = \sum_{i=1}^n \phi_{11}x_1 + \phi_{{21}}x_2 + \phi_{31}x_3 + .........\phi_{pi}x_p $$,
where n=no of observations, p = number of variables.It is a linear combination to find out the highest variance across data.
By normalized I mean $\sum_{j=1}^{p} \phi_{j1}^2 = 1$.

3) We refer to the weights $\phi_{pi}$ as __Loadings__.The loadings make up the principal components loading vector.
$$\phi_1 = (\phi_{11},\phi_{21},\phi_{31}......,\phi_{p1})^T$$ is the loadings vector for $PC_1$.

4) We constrain the loadings so that their sum of squares could be 1 , as otherwise setting these elements to be arbitarily large in absolute value could result in an arbitarily large variance.



The first Principal component solves the below optimization problem of maximizing variance across the components--

$$maximize: \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^p (\phi{ji}.X_{ij})^2  subject \ to \sum_{j=1}^p \phi_{ji}^2=1  $$
Here each principal component has mean 0.


