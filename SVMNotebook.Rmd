---
title: "Support Vector Machines"
output: html_notebook
---

This article will explain how to implement Support Vector Machines in R and their in depth interpretation.

SVM does not uses any Probability Model as such like other Classifiers use , because it directly looks for a Hyperplane which divides and sagments the data and classes.

General form of a Hyperplane is :

$$\beta_0  + \beta_1X_1  +  \beta_2X_2  + . . .. . \beta_pX_p = 0 $$
where $p$ is the number of Dimentions.


1) For $p=2$ i.e for a 2-D space it is a Line.

2)The vector $(\beta_1,\beta_2,\beta_3...\beta_p) is \ just \ a \ Normal\ vector.$ A vector in simple terms is just a 1-Dimentional Tensor or a 1-D array.



__Support Vector Classifiers__ are majorly used  for solving a binary clssification problem where we only have 2 class labels say $Y = [-1,1]$ and a bunch of predictors $X_i$ .And what SVM does is that it generates Hyperplanes which in simple terms are just __straight lines or planes__ or are Non-linear curves , and these lines are used to saperate the data or sagment the data into 2 categories or more depending on the type of Classification problem.

We try to find a __plane__ which saperates the classes in some feature space $X_i$.

Another concept in SVM is of *Maximal Margin Classifiers*.What it means is that amongst a set of separating hyperplanes SVM aims at finding the one which maximizes the margin $M$.This simply means that we want to maximize the gap or the distance between the 2 classes from the Decision Boundary(separating plane).

This concept of separating data linearly into 2 different classes using a Linear Separator or a straight linear line is called *__Linear Separability__*.

The term *__Support Vectors__* in SVM are the data points or training examples which are used to define or maximizing the margin.The support vectors are the points which are close to the decision boundary or on the wrong side of the boundary.  



-------------

###Linear SVM Classifier in R


```{r}
set.seed(10023)
#generating data
#a matrix with 20 rows and 2 columns
x=matrix(rnorm(40),20,2)
x
y=rep(c(-1,1),c(10,10))
x[y==1,]=x[y==1,]+1 #2 classes are [-1,1]

#plotting the points
plot(x,col=y+2,pch=19)

```




------------

#### Using the 'e1071' package to fit a SVM classifier

```{r,message=FALSE,warning=FALSE}
require(e1071)
#converting to a data frame
data<-data.frame(x,y=as.factor(y))
head(data)

svm<-svm(y ~ .,data=data,kernel="linear",cost=10,scale = F)
#here cost 'c' is a tuning parameter .The larger it is more stable the margin becomes, it is like a Regularization parameter 
svm
#so we have 10 support vectors

#plotting
plot(svm,data)
```




