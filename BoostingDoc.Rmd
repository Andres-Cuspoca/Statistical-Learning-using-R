---
title: "Boosting in R"
output: html_notebook
---


## Boosting

Random Forests actually used to reduce the variance of the Trees by averaging them. SO it generates big Bushy trees and then averages them to get rid of variance.

__Boosting__ on other hand generates smaller simpler trees and goes at the *__Bias__*.So the Idea in Boosting is to convert a *__Weak learner__* to a *__Strong Learner__* by doing *weighted averaging* of lots of Models generated on Harder Examples and using the Information from a previous Model.  

Harder Examples in the sense means the training Examples which were not classified correctly or more generally which were not predicted correctly by the Model.

Boosting is a Sequential Method. Each tree that's added into the mix is added to improve the perfomance of previous collection of Trees.



-----


###Implementing Gradient Boosting in R using gbm package


'gbm' package is the Gradient Boosting Package.

```{r,warning=FALSE,message=FALSE}
require(gbm)
require(MASS)

```


Building the Boosted Trees on Boston Housing Dataset.


```{r}

Boston.boost<-gbm(medv ~ . ,data = Boston[-train,],distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)
Boston.boost

summary(Boston.boost) #Summary gives a table of Variable Importance and a plot of Variable Importance


```

The above Boosted Model is a Gradient Boosted Model which generates 10000 trees and the shrinkage parameter $\lambda= 0.01$ which is also a sort of __Learning Rate__. Next parameter is the interaction depth which is the total *splits* we want to do.So here each tree is a small tree with only 4 splits.

The summary of the Model gives a *__Feature importance Plot__* . And the 2 most important features which explaines the maximum variance in the Data set is 'lstat' and 'rm'.


-----


###Let's plot the Partial Dependence Plots


The partial Dependence Plots will tell us the relationship and dependence of the variables with the Response variable.

```{r}



```

