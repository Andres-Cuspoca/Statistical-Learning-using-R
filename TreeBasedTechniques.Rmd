---
title: "Tree Based Methods in R"
output:
  html_document: default
  html_notebook: default
---




### This is a article on how to implement Tree based Techniques in R to do Predictive Modelling
 
It will include implementation of Decision Trees ,  Random Forests and other Tree based variants.



-----------



#### Loading the required Packages

```{r,warning=FALSE,message=FALSE}

require(ISLR) #package containing data
require(ggplot2)
require(tree)

#Using the Carseats data set 

attach(Carseats)
?Carseats


```


Carseats is a simulated data set containing sales of child car seats at 400 different stores.


```{r}
#Checking the distribution of Sales

ggplot(aes(x = Sales),data = Carseats) + 
  geom_histogram(color="black",fill = 'purple',alpha = 0.6, bins=30) + 
  labs(x = "Unit Sales in Thousands", y = "Frequency")


```
As the histogram suggests - It is Normally distributed
Highest frequency of around 8000 Unit Sales


```{r}

#Making a Factor variable from Sales

HighSales<-ifelse(Sales <= 8,"No","Yes")
head(HighSales)

#Making a Data frame
Carseats<-data.frame(Carseats,HighSales)


```


----------




## Fitting a Binary Classification Tree


Now we are going to fit a __Tree__ to the Carseats Data to predict if we are going to have High Sales or not.The __tree()__ function uses a *__Top-down Greedy__* approch to fit a Tree which is also known as *__Recursive Binary Splitting__*.It is Greedy because it dosen't finds the best split amongst all possible splits,but only the best splits at the immediate place its looking i.e the best Split at that particular step.

```{r}
#We will use the tree() function to fit a Desicion Tree
?tree

#Excluding the Sales atrribute
CarTree<-tree(HighSales ~ . -Sales , data = Carseats,split = c("deviance","gini"))
#split argument split	to specify the splitting criterion to use.

CarTree #Outputs a Tree with various Splits at different Variables and Response at Terminals Nodes
#The numeric values within the braces are the Proportions of Yes and No for each split.

#Summary of the Decision Tree
summary(CarTree)


```

The summary of the Model consists of the imporatant variables used for splitting the data which minimizes the deviance(Error) Rate and another Splitting criterion used is __Gini Indes__, which is also called *Purity Index*.



-------


#Plotting the Decision Tree


```{r}
plot(CarTree)
#Adding Predictors as text to plot
text(CarTree ,pretty = 1 )

```

This tree is quiet Complicated and hard to understand due to lots of Splits and lots of variables included in the predictor space.The leaf nodes consists of the Response value i.e __Yes / No __.


