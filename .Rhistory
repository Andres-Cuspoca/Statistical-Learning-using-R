plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,1))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(4,1))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,2))
par(mar=rep(2,4))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,6))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting as increase in Model Variance as no of Predictors Increase")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared on Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Overfitting Cases and increase in Model Variance as no of Predictors Increases")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared on Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
require(glmnet)
cv.glmnet
install.packages("glmnet")
require(glmnet)
?cv.glmnet
dim(Hitters)
263*2/2
263*2/3
263*1/3
trainrow<-sample(seq(263),180,replace = FALSE)
head(trainrow)
forw<-regsubsets(Salary ~ . ,data=Hitters[trainrow,] ,
method = "forward",nvmax =19)
require(leaps)
forw<-regsubsets(Salary ~ . ,data=Hitters[trainrow,] ,
method = "forward",nvmax =19)
val.error<-rep(NA,19)
?model.matrix()
x.test<-model.matrix(Salary ~ ., data = Hitters[-trainrow,])
View(Xy)
View(x.test)
dim(x.test)
for(i in 1:19) {
coefi = coef(forw , id = i)
pred = x.test[,names(coefi)]%*%coefi
val.error = mean((Hitters$Salary[-trainrow]-pred )^2)#Mean sqrd Error on Test set
}
coefi
for(i in 1:19) {
coefi = coef(forw , id = i)
pred = x.test[,names(coefi)]%*%coefi
val.error[i] = mean((Hitters$Salary[-trainrow]-pred )^2)#Mean sqrd Error on Test set
}
which.min(val.error)
val.error
plot(sqrt(val.error),pch=19,type='b',ylab="RMS error on Test Set",xlab="No of Predictors")
points(sqrt(forw$rss[-1]/180),type='b',pch=19,col='blue')
points(sqrt(forw$rss/180),type='b',pch=19,col='blue')
plot(sqrt(val.error),pch=19,type='b',ylab="RMS error on Test Set",xlab="No of Predictors")
points(sqrt(forw$rss[-1]/180),type='b',pch=19,col='blue')
summary(forw)
legend('topright',c("MSE on Test Set ","MSE on Training Data"),col=c("black","blue"))
plot(sqrt(val.error),pch=19,ylim=c(280,400),type='b',ylab="RMS error on Test Set",xlab="No of Predictors")
#also adding the Mean RSS omn training data
points(sqrt(forw$rss[-1]/180),type='b',pch=19,col='blue')
legend('topright',c("MSE on Test Set ","MSE on Training Data"),col=c("black","blue"))
legend('topright',pch=19,c("MSE on Test Set ","MSE on Training Data"),col=c("black","blue"))
plot(sqrt(val.error),pch=19,ylim=c(280,400),type='b',ylab="RMS error on Test Set",xlab="No of Predictors")
#also adding the Mean RSS omn training data
title("MSE on TEST SET vs MSE on TRAIN SET")
points(sqrt(forw$rss[-1]/180),type='b',pch=19,col='blue')
legend('topright',pch=19,c("MSE on Test Set ","MSE on Training Data"),col=c("black","blue"))
#AS expected the MSE in Training data decrease as the no of predictors
#in the Model Increases due to forward stepwise which adds 1 mor
plot(sqrt(val.error),pch=19,ylim=c(280,400),type='b',ylab="RMS error",xlab="No of Predictors")
#also adding the Mean RSS omn training data
title("MSE on TEST SET vs MSE on TRAIN SET")
points(sqrt(forw$rss[-1]/180),type='b',pch=19,col='blue')
legend('topright',pch=19,c("MSE on Test Set ","MSE on Training Data"),col=c("black","blue"))
head(pred)
predict.regsubset<-function(object,newdata,id,...)
{
form=as.formula(object$call[[2]])
mat<-model.matrix(form,newdata)#Test Set
coefi<-coef(object,id=id)# Model coefficients
#last line are the predicted values
mat[,names(coefi)]%*%coefi
}
set.seed(10)
?rep
rep(1)
rep(1,20)
rep(1:10,20)
rep(1:10,19)
folds=sample(rep(1:10,length = nrow(Hitters)))
folds
table(folds)
folds
cv.errors<-rep(NA,19)
cv.errors<-matrix(NA,10,19)
for(k in 1:10)
{
#fitting the Models on each k-1 folds
best.fit<-regsubsets(Salary ~ . , data = Hitters[folds!=k,],
nvmax=19,method='forward')
}
for(i in 1:19)
{
pred = predict.regsubset(best.fit,Hitters[folds==k,],
id=i)
cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
}
for(k in 1:10)
{
#fitting the Models on each k-1 folds
best.fit<-regsubsets(Salary ~ . , data = Hitters[folds!=k,],
nvmax=19,method='forward')
#predictions on left out Kth fold -Validation Set
for(i in 1:19)
{
pred = predict.regsubset(best.fit,Hitters[folds==k,],
id=i)
cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
}
}
head(cv.errors)
pred
pred
rmse.cv=sqrt(apply(cv.errors,2,mean))
rmse.cv
plot(rmse.cv,pch=19,type='b')
plot(rmse.cv,pch=19,type='b',ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors",title="CV error for each Submodel")
plot(rmse.cv,pch=19,type='b',ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
title("CV error for each Submodel")
best.fit
points(sqrt(best.fit$rss[-1]/180),pch=19,col='blue',type='b')
plot(rmse.cv,pch=19,type='b',ylim=c(320,400),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
points(sqrt(best.fit$rss[-1]/180),pch=19,col='blue',type='b')
title("CV error for each Submodel")
plot(rmse.cv,pch=19,type='b',ylim=c(320,420),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
points(sqrt(best.fit$rss[-1]/180),pch=19,col='blue',type='b')
title("CV error for each Submodel")
plot(rmse.cv,pch=19,type='b',ylim=c(320,460),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
points(sqrt(best.fit$rss[-1]/180),pch=19,col='blue',type='b')
title("CV error for each Submodel")
plot(rmse.cv,pch=19,type='b',ylim=c(320,440),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
points(sqrt(best.fit$rss[-1]/180),pch=19,col='blue',type='b')
title("CV error for each Submodel")
which.min(best.fit$rss)
plot(rmse.cv,pch=19,type='b',ylim=c(320,440),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
title("CV error for each Submodel")
plot(rmse.cv,pch=19,type='b',ylim=c(320,400),ylab="Root Mean squared Cross validation Error",
xlab="Number of Predictors")
title("CV error for each Submodel")
rep(10,10)
rep(1:10,10)
require(glmnet)
?glmnet
attach(Hitters)
ridge<-glmnet(x, y ,alpha = 0)
x = model.matrix(Salary~.-1 , data = Hitters)#Predictors
y = Hitters$Salary #Response Variable to be used in Linear Model
#First we will do Ridge Regression by setting alpha=0
ridge<-glmnet(x, y ,alpha = 0)#R
summary(ridge)
ridge
plot(ridge,xvar = "lambda",label = TRUE)
cv.ridge = cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
plot(cv.ridge,xlab="Log(lambda-Tuning Parameter)",ylab("MSE"))#all 20 variables in the Model-19 predictors + 1 intercept for each
title("RIDGE REGRESSION PLOT")
plot(cv.ridge,xlab="Log(lambda-Tuning Parameter)",ylab("MSE"))#all 20 variables in the Model-19 predictors + 1 intercept for each
plot(cv.ridge)
plot(ridge,xvar = "dev",label = TRUE)
lasso<-glmnet(x,y)
lasso
plot(lasso,xvar="lambda",label = T)
plot(lasso,xvar = "dev",label = T)
lasso.cv<-cv.glmnet(x,y)
plot(lasso.cv)
coef(lasso.cv)
trainrow<-sample(seq(263),180,replace = FALSE)
head(trainrow)
lasso.val<-glmnet(x[trainrow,],y[-trainrow])
y[-trainrow]
lasso.val<-glmnet(x[trainrow,],y[trainrow])
sample(20,10)
sample(1:20,10)
sample(1:20,10,replace = T)
sample(1:20,10,replace = F)
plot(lasso.val,xvar = 'dev',label = T)
pred=predict(lasso.val,x[-trainrow,])#Predictions on Test set
pred
dim(pred)
rmse = sqrt(apply((y[-trainrow]-pred)^2),2,mean)
rmse = sqrt(apply((y[-trainrow]- pred)^2 ,2 ,mean))
plot(log(lasso.val$lambda),rmse,type='b',pch=19)
plot(log(lasso.val$lambda),rmse,type='b')
plot(log(lasso.val$lambda),rmse,type='b',pch=19,col='red')
plot(log(lasso.val$lambda),rmse,type='b',pch=19,col='red',
xlab="Root Mean Square Error on Validation Set",ylab=("Log of Lambda(Tuning Parameter"))
title("Lasso Implementation")
plot(log(lasso.val$lambda),rmse,type='b',pch=19,col='red',
ylab="Root Mean Square Error on Validation Set",xlab=("Log of Lambda(Tuning Parameter)"))
title("Lasso Implementation")
lam.best<-which.min(lasso.val$lambda)
lam.best
lam.best<-lasso.val$lambda[order(rmse)[1]]
lam.best
coef(lasso.val,s=lam.best)
lam.best<-lasso.val$lambda[order(rmse)]
lam.best
lam.best<-lasso.val$lambda[order(rmse)[1]]
lam.best
coef(lasso.val,s=241)
coef(lasso.val,s=241.8018890)
coef(lasso.val,s=lam.best)
log(6)
log(6.4224)
#Package containeing the Dataset
require(ISLR)
attach(Wage)#Datset
require(ISLR)
attach(Wage)
mod<-lm(wage~ploy(age,4),data =Wage)
mod<-lm(wage~poly(age,4),data =Wage)
summary(mod)
agelims<-range(age)
agelims<-range(age)
agelis
agelims
age.grid<-seq(from=agelims[1], to = agelims[2])
pred<-predict(mod,newdata = list(age=age.grid),se=TRUE)
pred
pred$se.fit
pred$fit
se.tab<-cbind(pred$fit+2*pred$se.fit,pred$fit -  2*pred$se.fit)
se
se.tab
plot(age,Wage,col="darkgrey")
plot(age,wage,col="darkgrey")
lines(age.grid,pred$fit,col="blue",lwd=2)
plot(age,wage,col="darkgrey")
lines(age.grid,pred$fit,col="blue",lwd=2)
matlines(x=age.grid,y=se.tab,lwd=2,col="blue")
matlines(x=age.grid,y=se.tab,lwd=2,col="blue")
plot(age,wage,col="darkgrey")
#Plotting the age values vs Predicted Wage values for those Ages
lines(age.grid,pred$fit,col="blue",lwd=2)
#To plot the Error bands
matlines(x=age.grid,y=se.tab,lwd=2,col="blue")
matlines(x=age.grid,y=se.tab,lty =2,col="blue")
plot(age,wage,col="darkgrey")
#Plotting the age values vs Predicted Wage values for those Ages
lines(age.grid,pred$fit,col="blue",lwd=2)
#To plot the Error bands
matlines(x=age.grid,y=se.tab,lty =2,col="blue")
fit2<-lm(wage ~ age + I(age^2) + I(age^3) + I(age^4),data = Wage)
summary(fit2)
plot(fitted(mod),fitted(fit2))
plot(fitted(mod),fitted(fit2),xlab="First Polynomial Model",ylab="Polynomial Model wrapped inside Identity function", main="Fitted values of Both models are exactly same")
#This time we will use different basis of polynomials
fit2<-lm(wage ~ age + I(age^2) + I(age^3) + I(age^4),data = Wage)
summary(fit2)
plot(fitted(mod),fitted(fit2),xlab="First Polynomial Model",ylab="Polynomial Model wrapped inside Identity function", main="Fitted values of Both models are exactly same")
mod1<-lm(wage ~ education , data = Wage)
mod2<-lm(wage ~ education + age,data = Wage)
mod3<-lm(wage ~ education + age + poly(age,2),data = Wage)
mod4<-lm(wage,  eduaction + age + poly(age,3),data = Wage)
#Making Nested Models-i.e Each Next Model includes previous Model and is a special case for previous one
mod1<-lm(wage ~ education , data = Wage)
mod2<-lm(wage ~ education + age,data = Wage)
mod3<-lm(wage ~ education + age + poly(age,2),data = Wage)
mod4<-lm(wage,  eduaction + age + poly(age,3),data = Wage)
#Making Nested Models-i.e Each Next Model includes previous Model and is a special case for previous one
mod1<-lm(wage ~ education , data = Wage)
mod2<-lm(wage ~ education + age,data = Wage)
mod3<-lm(wage ~ education + age + poly(age,2),data = Wage)
mod4<-lm(wage ~  eduaction + age + poly(age,3),data = Wage)
#Making Nested Models-i.e Each Next Model includes previous Model and is a special case for previous one
mod1<-lm(wage ~ education , data = Wage)
mod2<-lm(wage ~ education + age,data = Wage)
mod3<-lm(wage ~ education + age + poly(age,2),data = Wage)
mod4<-lm(wage ~ education + age + poly(age,3),data = Wage)
anova(mod1,mod2,mod3,mod4)
plot(age,wage)
pred1<-predict(mod4,newdata = list(age=age.grid),se=TRUE)
par(mfrow=c(2,2))
plot(mod4)
plot(mod2)
plot(mod3)
plot(mod1)
logmod<-glm(I(wage > 250 ) ~ poly(age,3),data = Wage , family = "binomial")
summary(logmod)
BIC(mod1,mod2,mod3,mod4)
pred2<-predict(logmod,newdata = list(age=age.grid),se=TRUE)
se.band<-pred$fit + cbind(fit=0,lower=-2*pred$se.fit , upper = 2*pred$se.fit )
se.band
se.band<-pred2$fit + cbind(fit=0,lower=-2*pred2$se.fit , upper = 2*pred2$se.fit )
se.band
se.band[1:5,]
pred2<-predict(logmod,newdata = list(age=age.grid),se=TRUE,type = "class")
pred2<-predict(logmod,newdata = list(age=age.grid),se=TRUE,type = "prob")
pred2<-predict(logmod,newdata = list(age=age.grid),se=TRUE,type = "response")
pred2
head(pred2)
se.band<-pred2$fit + cbind(fit=0,lower=-2*pred2$se.fit , upper = 2*pred2$se.fit )
se.band[1:5,]
pred2<-predict(logmod,newdata = list(age=age.grid),se=TRUE)
#Standard Error Bands
#a Matrix with 3 columns
se.band<-pred2$fit + cbind(fit=0,lower=-2*pred2$se.fit , upper = 2*pred2$se.fit )
se.band[1:5,]
prob.bands = exp(se.band)/ (1 + exp(se.band))
prob.bands
predict(logmod,newdata = list(age=age.grid),se=TRUE,type="response")
predict(logmod,newdata = list(age=age.grid),se=TRUE,type="response")
matplot(age.grid,prob.bands,col="green",lwd = c(2,2,2),lty=c(1,1,2),
type="l",ylim=c(0,.1))
matplot(age.grid,prob.bands,col="green",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1))
matplot(age.grid,prob.bands,col="blue",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1))
points(jitter(age),I(wage > 250)/10 , pch="1",cex=0.5)
matplot(age.grid,prob.bands,col="blue",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1))
#jitter() function to uniformly add random noise to properly see the densities
points(jitter(age),I(wage > 250)/10 , pch="1",cex=0.5)
matplot(age.grid,prob.bands,col="blue",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1))
#jitter() function to uniformly add random noise to properly see the densities
points(jitter(age),I(wage > 250)/10 , pch="2",cex=0.5)
matplot(age.grid,prob.bands,col="blue",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1))
#jitter() function to uniformly add random noise to properly see the densities
points(jitter(age),I(wage > 250)/10 , pch="I",cex=0.5)
matplot(age.grid,prob.bands,col="blue",lwd = c(2,2,2),lty=c(1,2,2),
type="l",ylim=c(0,.1),xlab="Age",ylab="Probability Values")
#jitter() function to uniformly add random noise to properly see the densities
points(jitter(age),I(wage > 250)/10 , pch="I",cex=0.5)
