cv.error10 = rep(0,5)
for(d in degree)
{
model<-glm(mph~poly(horsepower,d),data = Auto)
cv.error10[d] = cv.glm(Auto, model, k=10)$delta[1]
}
cv.error10 = rep(0,5)
for(d in degree)
{
model<-glm(mpg~poly(horsepower,d),data = Auto)
cv.error10[d] = cv.glm(Auto, model, k=10)$delta[1]
}
plot(degree, cv.error10,type='b',col='red')
cv.error10 = rep(0,5)
for(d in degree)
{
mod<-glm(mpg~poly(horsepower,d),data = Auto)
cv.error10[d] = cv.glm(Auto, mod, k=10)$delta[1]
}
cv.error10 = rep(0,5)
for(d in degree)
{
mod<-glm(mpg~poly(horsepower,d),data = Auto)
cv.error10[d] = cv.glm(Auto, mod, K=10)$delta[1]
}
plot(degree, cv.error10,type='b',col='red')
lines(x = degree , y = cv.error,type='b' ,title="Cross validation error for different degrees",xlab = "Degree",ylab = "Cross validation error")
plot(degree, cv.error10,type='b',col='red')
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",y="10 fold CV error")
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",y="10 fold CV error")
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",ylab ="10 fold CV error")
mod2<-glm(mpg ~ horsepower^2, data = Auto)
cv.glm(Auto,mod2,K=10)$delta
mod2<-glm(mpg ~ poly(horsepower,2), data = Auto)
cv.glm(Auto,mod2,K=10)$delta
mod2<-glm(mpg ~ poly(horsepower,4), data = Auto)
cv.glm(Auto,mod2,K=10)$delta
mod2<-glm(mpg ~ poly(horsepower,2), data = Auto)
cv.glm(Auto,mod2,K=5)$delta
cv.error5 = rep(0,5)
for(d in degree)
{
mod<-glm(mpg ~ poly(horsepower,d),data =Auto)
cv.error5[d] = cv.glm(Auto , mod , K=5)$delta[1]
}
lines(degree, cv.error5 , col = 'blue')
lines(degree, cv.error5 , col = 'blue',type= 'b')
legend(2000, 9.5, c('10-fold CV , 5-fold CV',lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue")))
lines(degree, cv.error5 , col = 'blue',type= 'b')
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",ylab ="10 fold CV error")
lines(degree, cv.error5 , col = 'blue',type= 'b')
legend(2000, 9.5, c('10-fold CV , 5-fold CV',lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue")))
legend(2000, 9.5, c('10-fold CV , 5-fold CV',lty=c(1,1),lwd=c(2.5,2.5),col=c("red","blue")))
legend(2000, 9.5, c( "5-fold CV"),lty=c(1),lwd=c(2.5),col=c("blue")))
legend(2000, 9.5, c( "5-fold CV"),lty=c(1),lwd=c(2.5),col=c("blue"))
lines(degree, cv.error5 , col = 'blue',type= 'b')
lines(degree, cv.error5 , col = 'blue',type= 'b')
legend(2000, 9.5, c( "5-fold CV"),lty=c(1),lwd=c(2.5),col=c("blue"))
lines(degree, cv.error5 , col = 'blue',type= 'b'
)
lines(degree, cv.error5 , col = 'blue',type= 'b')
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",ylab ="10 fold CV error vs 5 fold")
lines(degree, cv.error5 , col = 'blue',type= 'b')
legend(2000, 9.5, c( "5-fold CV"),lty=c(1),lwd=c(2.5),col=c("blue"))
load("C:/Users/hp/Downloads/5.R.RData")
View(Xy)
mod<-glm(y ~ X1 + X2)
mod<-glm(y ~ X1 + X2,data = Xy)
summary(mod)
matplot(Xy,type='l')
matplot(Xy,type='i')
matplot(Xy,type='L')
matplot(Xy,type='i')
source('F:/PROJECTS/StatsLearn/Bootstrapping.R', echo=TRUE)
matplot(Xy,type='l')
require(ISLR)
summary(Hitters)
glimpse(Hitters)
require(dplyr)
glimpse(Hitters)
str(Hitters)
?Hitters
Hitters<-na.omit(Hitters)
View(Hitters)
is.na(Hitters)
sum(is.na(Hitters))
install.packages(leaps)
install.packages("leaps")
require(leaps)
?leaps
attach(Hitters)
reggfit.full<-regsubsets(Salary ~ . , data = Hitters )
reggfit.full
summary(reggfit.full)
reggfit.full$nvmax
reggfit.full$rss
reggfit.full$bic
summary(reggfit.full$bic)
reggfit.full$adjr2
reggfit.full$rsq
reggfit.full$cp
cp(reggfit.full)
reggfit.full<-regsubsets(Salary ~ . , data = Hitters ,nbest = 4 )
summary(reggfit.full)
reggfit.full<-regsubsets(Salary ~ . , data = Hitters ,nbest = 2 )
reggfit.full
summary(reggfit.full)
reggfit.full<-regsubsets(Salary ~ . , data = Hitters )
summary(reggfit.full)
names(reggfit.full)
reggfit.full$ress
reggfit.full$sserr
summary(reggfit.full)=Modsumm
summary(reggfit.full)->Modsumm
Modsumm$rsq
Modsumm$rss
Modsumm$adjr2
Mod2<-regsubsets(Salary ~ . , data = Hitters , nvmax = 19)
summod2<-summary(Mod2)
summod2
plot(reggfit.full,scale = 'cp')
plot(reggfit.full,scale = 'Cp')
plot(Mod2,scale="bic")
plot(Mod2,scale="Cp")
plot(Mod2,scale="Cp",xlab="Variables" , ylab =" Cp Statistic, lesser the better Model")
plot(summod2$cp,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
points(10,summod2$cp[10],pch=20,col='blue')
plot(summod2$adjr2,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
par(mfrow=c(1,2))
plot(summod2$cp,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
points(10,summod2$cp[10],pch=20,col='blue')#coloring the Best Model with 10 predictors
plot(summod2$adjr2,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
points(10,summod2$cp[10],pch=20,col='blue')
max(summod2$adjr2)
which.max(summod2$adjr2)
points(11,summod2$cp[11],pch=20,col='blue')
plot(summod2$adjr2,xlab="Number of Variables", ylab = "Adjusted R-squared,Larger the better")
which.max(summod2$adjr2)
points(11,summod2$cp[11],pch=20,col='blue')
plot(summod2$cp,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
points(10,summod2$cp[10],pch=20,col='blue')#coloring the Best Model with 10 predictors
plot(summod2$adjr2,xlab="Number of Variables", ylab = "Adjusted R-squared,Larger the better")
which.max(summod2$adjr2)
points(11,summod2$cp[11],pch=20,col='blue')
plot(summod2$cp,xlab="Number of Variables", ylab = "Cp statistic-Lesser The better")
points(10,summod2$cp[10],pch=20,col='blue')#coloring the Best Model with 10 predictors
plot(summod2$adjr2,xlab="Number of Variables", ylab = "Adjusted R-squared,Larger the better")
which.max(summod2$adjr2)
points(11,summod2$cp[11],pch=20,col='blue')
which.min(summod2$adjr2)
which.min(summod2$cp)
coef(summod2$cp[10])
coef(Mod2[10])
coef(Mod2$cp[10])
coef(Mod2,id = 10)
coef(Mod2,10)
forwmod<-regsubsets(Salary ~ . , data  = Hitters , method = 'forward',nvmax = 19 )
sumfor<-summary(forwmod)
sumfor
sumfor$rsq
sumfor$cp
sumfor$bic
sumfor$adjr2
which.max(sumfor$adjr2)
sumfor$outmat
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIS statistic")
title("Forward Stepwise Selection")
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIS statistic")
title("Forward Stepwise Selection")
title("Forward Stepwise Selection")
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIS statistic")
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
title("Forward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIS statistic")
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
title("Forward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIC statistic")
which.min(sumfor$bic)
which.max(sumfor$adjr2)
points(6,sumfor$bic[6],pch=20,col='red')
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
title("Forward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIC statistic")
points(6,sumfor$bic[6],pch=20,col='red')
points(11,sumfor$adjr2[11],pch=20,col='blue')
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
points(11,sumfor$adjr2[11],pch=20,col='blue')
title("Forward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIC statistic")
points(6,sumfor$bic[6],pch=20,col='red')
which.max(sumfor$adjr2)
sumfor$adjr2
max(sumfor$adjr2)
sumfor$adjr2
sumfor$bic
class(sumfor)
as.data.frame(sumfor)
backmod<-regsubsets(Salary ~ . , data  = Hitters , method = 'backward ', nvmax = 19 )
backmod<-regsubsets(Salary ~ . , data  = Hitters , method = 'back', nvmax = 19 )
backsum<-summary(backmod)
backsum
backsum$rss
which.min(backsum$rss)
backsum$adjr2
which.max(backsum$adjr2)
which.min(backsum$cp)
which.min(backsum$bic)
which.max(backsum$adjr2)
which.min(backsum$bic)
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
points(11,sumfor$adjr2[11],pch=20,col='blue')
#Adjst R-squred is highest for Model with 11 predictors
title("Backward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIC statistic")
points(8,sumfor$bic[8],pch=20,col='red')
#BIC is least for a model with 8 predictors
coef(backmod,id=8)
predict(backmod[8],newdata  )
plot(sumfor$adjr2,xlab="Number of Variables" , ylab =  " Adjusted R-squared")
points(11,sumfor$adjr2[11],pch=20,col='green')
#Adjst R-squred is highest for Model with 11 predictors
title("Backward Stepwise Selection")
plot(sumfor$bic,xlab='Number of Predictors '  , ylab = "BIC statistic")
points(8,sumfor$bic[8],pch=20,col='yellow')
#BIC is least for a model with 8 predictors
plot(backsum$rss,xlab="Number of Variables",ylab = "Residual Sum of Squared Error on Training Data")
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Residual Sum of Squared Error on Training Data")
par(mfrow=c(1,1))
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col='blue',type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col='blue',type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col='blue',type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,col='blue',type='b')
par(mfrow=c(1,1))
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
lines(backsum$bic,col='blue',type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,col='blue',pch=19,type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,col='blue',pch=19,type='b')
plot(backsum$rss/nrow(Hitters),xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$rss,col='blue',pch=19,type='b')
plot(backsum$rss/nrow(Hitters),type='b',xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$rss,col='blue',pch=19,type='b')
points(backsum$rss,col='blue',pch=19,type='b')
par(mfrow=c(1,1))
plot(backsum$rss/nrow(Hitters),type='b',xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,col='red',pch=19,type='b')
?points
points(backsum$bic,col='red',pch=19,type='b')
par(mfrow=c(1,1))
plot(backsum$rss/nrow(Hitters),type='b',xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,col='red',pch=19,type='b')
points(backsum$bic,y = NULL,col='red',pch=19,type='b')
points(backsum$bic,y = NULL,col='red',pch=19,type='l')
points(backsum$bic,y = NULL,col='red',pch=19,type='h')
points(backsum$bic,y = NULL,col='red',pch=19,type='b')
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y = NULL,col='red',pch=19,type='b')
points(backsum$bic,y = NULL,col='red',pch=19,type='b')
points(backsum$bic,y = NULL,col='red',pch=20,type='b')
plot(backsum$rss/nrow(Hitters),type='b',pch=21,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y = NULL,col='red',pch=20,type='b')
plot(backsum$rss/nrow(Hitters),type='b',pch=22,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y = NULL,col='red',pch=20,type='b')
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
length(backsum$bic)
plot(x = degree , y = cv.error,type='b' ,title="Cross validation error for different degrees",xlab = "Degree",ylab = "Cross validation error")
plot(degree, cv.error10,type='b',col='red',xlab="Degree of polynomial",ylab ="10 fold CV error vs 5 fold")
#Hence we can see that Model with quadratic degree is the best one with least
lines(degree, cv.error5 , col = 'blue',type= 'b')
legend("topright", c("5-fold CV","10-fold CV"),col=c("blue","red"),pch=19)
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),y=NULL,type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),y=NULL,type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y=NULL,pch=19,col="blue",type='b')
lines(backsum$bic,y=NULL,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),y=NULL,type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y=NULL,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),y=NULL,type='p',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y=NULL,pch=19,col="blue",type='p')
plot(backsum$rss/nrow(Hitters),y=NULL,type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,y=NULL,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),y=NULL,type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$adjr2,y=NULL,pch=19,col="blue",type='b')
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
points(backsum$bic,pch=19,col="blue",type='b')
par(mfrow=c(2,1))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
plot(backsum$bic,type='b',pch=19,xlab="Number of variables",
ylab="BIC value")
par(mfrow=c(1,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",ylab = "Mean Squared Error on Training Data")
plot(backsum$bic,type='b',pch=19,xlab="Number of variables",
ylab="BIC value")
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,1))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(4,1))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of variables",
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of variables",
ylab="R-squared in Training data")
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,2))
par(mar=rep(2,4))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,6))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=rep(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mar=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
par(mfrow=c(2,2))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared in Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Standard Cases of Overfitting as increase in Model Variance as no of Predictors Increase")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared on Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
plot(backsum$rss/nrow(Hitters),type='b',pch=19,xlab="Number of Variables",
ylab = "Mean Squared Error on Training Data")
title("Overfitting Cases and increase in Model Variance as no of Predictors Increases")
plot(backsum$bic,type='b',pch=19,col="blue",xlab="Number of Variables",
ylab="BIC value")
plot(backsum$rsq,type="b",pch=19,col='red',xlab="Number of Variables",
ylab="R-squared on Training data")
plot(backsum$adjr2,type='b',col='green',pch=19,xlab="Number of Variables",
ylab=('Adjusted R-squared'))
require(glmnet)
cv.glmnet
install.packages("glmnet")
require(glmnet)
?cv.glmnet
